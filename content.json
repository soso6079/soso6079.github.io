{"posts":[{"title":"4주차 회고록(NH 공모전 및 도메인 기초)","text":"아쉽게 끝난 공모전본격적으로 NH 공모전과 병행하며 보낸 한 주였다. 월요일이 휴일이라 오프라인으로 모여 하루종일 코딩하며 하루를 보냈고 그 후로도 저녁 9시부터 새벽까진 따로 공모전을 진행했다. 다만 가설 검증을 먼저 빠르게 해보고 진행해야 한다는 점을 잊고, 틀린 가설로 계속해서 분류를 시도했다는걸 제출 이틀 전에 알았다.난 금요일부터 밤새 달리고 주말도 달려서 제출하고 싶었지만.. 분석 경험이 많은 팀원 분이 멈출 줄 아는 것도 용기라고 말씀하셔서 깨끗하게 포기했다. 당장 공모전에만 매달릴 수 있는 상황도 아니었고 달린다고 좋은 결과가 나온다는 보장도 없었기 때문에 순순히 패배를 인정할 수 밖에 없었다. 대신 다같이 회고록을 적어보자는 말이 나왔고, 화면 공유를 하며 분석 과정과 서로 느낀 점을 공유했다. 이게 또 새로운 경험이었는데 정말 도움이 많이 됐다. 단순히 실패한 공모전으로 끝나버리진 않았다. 빡세다 빡세 부캠문제는 부스트캠프였다. 매주 힘들다고 느끼지만 매주 새롭게 빡세다. 도메인 기초 이론과 데이터 시각화 강의를 동시에 배우는 한주였는데, 데이터 시각화는 거의 못봤다. 한주동안 뮤트해버린 안수빈 마스터님께 진심으로 죄송..🙇 추천 시스템은 3강까지 알고 있던 내용들이라 조금 만만하게 봤다. 아 확실히 기초적인거 배우는구나, 싶었는데 기초적인건 나였고ㅋㅋ 본격적으로 모델 기반 추천 시스템이 나오고 잘 이해가 되지 않는 부분들이 나왔다. 이해가 안된다고 붙잡고 있는 스타일은 아니라 일단 넘기고 과제로 넘어갔다. 제대로 보지도 못한 과제과제는 그 전에 비하면 어렵지 않았는데 계속 연산 과정에서 차원을 못맞춰서 에러가 계속 떴다. 게다가 공모전과 국민취업지원센터 상담 덕분에 시간도 없어서 과제 2는 간단하게 밖에 못봤다. 안그래도 약한 파이토치 구현 부분이었는데, 조금밖에 보지 못한게 아쉬웠다. 이 글을 작성하고 우선 제대로 보지 못한 과제들부터 들여다볼 생각이다. 대학원 컨택이 생각나는 구팀..이제 본격적으로 팀이 꾸려지는 기간인게 느껴진다. 이미 팀을 꾸린 사람들도 꽤 있었다. 솔직히 어느 사람을 만나든 여기 있는 사람들 다 열정도 있고 열심히 할 걸 안다. 시험까지 봐가면서 지원한 곳이니까. 다만 상황이 다르면 마음가짐이 달라진다. 다른 사람들은 모르겠고 일단 나는 그렇다. 난 여전히 절박한 마음을 갖고 있다. 스트레스를 너무 받지 않으려고 과몰입으로는 안넘어가게 노력중이지만.. 만약 꾸려진 팀이 아직 졸업까지 한참 남은 사람이거나 대학원만 희망하는 사람들 밖에 없다면 솔직히 준내 스트레스 받을 것 같다.. 안그래도 하루죙일 집에 갇혀 있으니, 묘한 외로움도 스멀스멀 올라오고 앉았는데. 암튼 이런 상황을 아무것도 안해보고 맞이하긴 싫었다. 할 건 다 해봐야징. 내 구팀 글도 보기 좋게 수정하고 구팀 글을 하나하나 읽어보며 리스트 업했다. 그리고 계속 눈이 가던 사람에게 먼저 DM을 보내놓고 이 글을 쓰고 있다. 끝까지 긍정적인 답장을 못받았던 대학원 컨택 때가 계속 떠오르는 한 주다. 키워드 Apriori 알고리즘 Youtube Recommendation 논문 CDAE (Collaborative Denoising Auto-Encoders for Top-N Recommender systems) 논문 One-Shot Learning","link":"/2022/10/15/4%EC%A3%BC%EC%B0%A8%20%ED%9A%8C%EA%B3%A0%EB%A1%9D(NH%20%EA%B3%B5%EB%AA%A8%EC%A0%84%20%EB%B0%8F%20%EB%8F%84%EB%A9%94%EC%9D%B8%20%EA%B8%B0%EC%B4%88)/"},{"title":"5주차 회고록(구팀)","text":"부캠 시작하고 첫 여유이번 한주는 저번주에 비해 여유로웠다. 공모전도 끝났고 강의 분량도 적었다. 깃허브 특강과 level 2 팀 구성 때문인지 분량을 일부러 많이 안준 것 같았다. 아 추천 시스템만 그랬을 수 있다. 다른 도메인들은 어땠는지 모르겠다.도메인 추천 시스템은 한 강의에서 컨셉에 대한 모델의 발전을 쭉 보여주는 식이다. 그러다보니 영상은 40분 내외인데 배우는 모델은 3~4개씩 된다. 깊이도 꽤 깊게 들어가서 40분 영상이면 1시간 반은 보게 된다. 강의 퀄리티가 굉장히 좋아서 얕지 않은 수준으로 모델들을 알 수 있는데, 확실히 논문을 같이 보면서 정리할 수 있다면 더 좋을 것 같다. 사실 논문을 계속 보고 싶었지만, 시간 상의 이유로 미뤄왔는데 이번 주말에 Factorization Machine 논문을 읽어볼 생각이다. 정리까지 할 수 있을진 모르겠지만. ML Ops 공부 시작공모전이 끝나서 여유가 생기니까 전부터 하고 싶던 Ops를 시작하려 한다. 모두의 ML Ops(https://mlops-for-all.github.io)를 읽으면서 도커부터 설치했다. 도커와 쿠버네티스는 꼭 ops가 아니어도 쓸 줄 알아야 한다고 생각했는데 가볍게라도 익히고 가야겠다. 기술 블로그 맞나?한 주 동안 배운 걸 정리하고 싶어서 시작한 블로그였는데, 강의를 소화하는 것만으로도 시간이 부족해서 정리를 할 수가 없다. 결국 회고록만 적고 있는데 그래도 이렇게 정리해두는 게 의미있다고 생각한다. 적으면서 주간 학습 정리 게시판이 있던게 생각나서 한번 들어가봤는데, 다른 사람들도 간단하게 정리만 하는 수준으로 하고 있었다. 확실히 남들에게 어떻게 보일지 신경쓰면서 정리하긴 다들 어려운 듯 하다. 키워드 torch.mm / torch.matmul / torch.mul 차이 모두의 ML Ops (https://mlops-for-all.github.io)","link":"/2022/10/23/5%EC%A3%BC%EC%B0%A8%20%ED%9A%8C%EA%B3%A0%EB%A1%9D(%EA%B5%AC%ED%8C%80)/"},{"title":"torch.mm&#x2F;torch.matmul&#x2F;torch.mul 비교","text":"다양한 torch의 행렬곱 메소드PyTorch에서는 행렬곱에 대한 메소드가 여러가지 있다. torch.mm, torch.matmul, torch.mul 각각 어떤 차이점이 있는지 알아보자 torch.mm torch.mm(input, mat2, *, out=None) → Tensor torch.mm은 브로드캐스팅이 일어나지 않는 행렬 곱연산이다. 즉, 기본적으로 연산이 가능한 shape일 때 곱이 일어나고 가능하지 않은 형태일 땐 에러를 일으킨다.input이 $(n \\times m)$이고 mat2가 $(m \\times p)$일 때 결과는 $(n \\times p)$형태의 텐서가 출력된다. 브로드캐스팅이 예상하지 못한 곳에서 일어나면 어디서 문제가 일어났는지 파악하기 힘들어진다. 디버깅할 때 편하려면 미리 텐서를 squeeze나 view 등으로 형태를 맞춰주고 torch.mm을 이용하는게 속 편하다. 123456789101112131415&gt;&gt;&gt; mat1 = torch.randn(2, 3)&gt;&gt;&gt; mat1tensor([[ 1.5311e+00, -1.2978e-03, -1.3174e-01], [ 3.7436e-01, 1.7078e-01, -1.4868e+00]])&gt;&gt;&gt; mat2 = torch.randn(3, 3)&gt;&gt;&gt; mat2tensor([[ 1.5622, -1.6235, -1.5933], [ 0.5857, -0.6801, 0.9272], [ 0.0267, -0.1720, -2.5046]])&gt;&gt;&gt; torch.mm(mat1, mat2)tensor([[ 2.3875, -2.4621, -2.1107], [ 0.6451, -0.4682, 3.2857]]) torch.matmul torch.matmul(input, other, *, out=None) → Tensor torch.matmul은 broadcasting이 일어나는 행렬 곱연산이다. 즉 형태가 맞지 않아도 곱연산이 진행된다. 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; # vector x vector&gt;&gt;&gt; tensor1 = orch.randn(3)&gt;&gt;&gt; tensor2 = torch.randn(3)&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()torch.Size([])&gt;&gt;&gt; # matrix x vector&gt;&gt;&gt; tensor1 = torch.randn(3, 4)&gt;&gt;&gt; tensor2 = torch.randn(4)&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()torch.Size([3])&gt;&gt;&gt; # batched matrix x broadcasted vector&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)&gt;&gt;&gt; tensor2 = torch.randn(4)&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()torch.Size([10, 3])&gt;&gt;&gt; # batched matrix x batched matrix&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()torch.Size([10, 3, 5])&gt;&gt;&gt; # batched matrix x broadcasted matrix&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)&gt;&gt;&gt; tensor2 = torch.randn(4, 5)&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()torch.Size([10, 3, 5]) torch.mul torch.mul(input, other, *, out=None) → Tensor torch.mul은 element-wise($\\odot$) 등의 연산을 할 때 사용한다. 그래서 위 두 연산과 달리 other 파라미터에 tensor가 아닌 다른 자료형을 사용할 수 있다. 1234567891011121314151617181920&gt;&gt;&gt; a = torch.randn(3)&gt;&gt;&gt; atensor([ 0.2015, -0.4255, 2.6087])&gt;&gt;&gt; torch.mul(a, 100)tensor([ 20.1494, -42.5491, 260.8663])&gt;&gt;&gt; b = torch.randn(4, 1)&gt;&gt;&gt; btensor([[ 1.1207], [-0.3137], [ 0.0700], [ 0.8378]])&gt;&gt;&gt; c = torch.randn(1, 4)&gt;&gt;&gt; ctensor([[ 0.5146, 0.1216, -0.5244, 2.2382]])&gt;&gt;&gt; torch.mul(b, c)tensor([[ 0.5767, 0.1363, -0.5877, 2.5083], [-0.1614, -0.0382, 0.1645, -0.7021], [ 0.0360, 0.0085, -0.0367, 0.1567], [ 0.4312, 0.1019, -0.4394, 1.8753]]) Reference:https://pytorch.org/docs/stable/generated/torch.mm.htmlhttps://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmulhttps://pytorch.org/docs/stable/generated/torch.mul.html?highlight=torch%20mul#torch.mul","link":"/2022/10/23/torch%20%ED%96%89%EB%A0%AC%20%EA%B3%B1%20%EB%A9%94%EC%86%8C%EB%93%9C%20%EB%B9%84%EA%B5%90/"},{"title":"부스트코스 1주차 회고","text":"부스트코스를 진행한지 벌써 1주가 지났다. 프리코스에서 미리 공개됐던 강의들을 그대로 듣는 주차였기 때문에 부스트코스 자체만으로는 촉박하지 않았다. 다만 진행하던 파이토치 스터디도 있었고 내가 약하다고 생각한 AI Math 부분의 강좌는 다시 한번 더 들었기 때문에 여유롭지 않은 한 주였다. 게다가 심화과제는 생각한 것 보다 훨씬 어려웠다. 손도 못댄 문제들이 많았는데, 라이브러리를 모르거나 수식에 겁먹어서 필요 이상으로 쫄았던 부분도 있었다. 문제는 정말 문제 자체를 이해하지 못해서 풀지 못한 부분도 있었다는 점이다. 팀원들과도 이야기 했을 때 나만 그렇게 느낀 건 아니었다. 다행히 금요일에 오피스 아워를 통해 조교분들의 해설을 들을 수 있었다. 부스트코스를 한 주 동안 진행하면서 신청하길 잘했다고 생각이 든 부분이 많았다. 정말 질 좋은 강의와 과제, 운영진들의 케어, 심화과제에 대한 깔끔한 해설까지. 한달에 백단위씩 내고 다녔던 재수학원이 생각나던 한 주였다. 강의와 과제AI Math는 필수적으로 알아야할 키워드와 각 개념이 어떻게 AI에 쓰이는지 깔끔하게 정리해준다. 필요성과 깊이가지 챙겨갈 수 있는 좋은 강의였다. 난이도도 일부러 굳이 낮추지 않은 느낌이었는데 그 점이 오히려 좋았다. (물론 거의 이해 못한 채로 마무리하긴 했다.) 과제 역시 대학에서도 평 좋은 교수님의 과제를 보는 듯 했다. 그저 강의를 보기만 한게 아니라, 제대로 이해했는지 확인해볼 수 있는 내용이었다. 운영진의 케어부스트코스를 신청했을 때의 내 마음가짐은 어땠는지 잊지 않고 싶다. 잘한 것, 좋았던 것, 계속할 것잘못했던 것, 아쉬운 것, 부족한 것도전할 것, 시도할 것키워드 캐글 노트북 코랩보다 안정성이나 런타임 측면에서 더 좋다고 한다. 또 데이터를 업로드하고 다운로드하는 시간이 굉장히 빠르다. 캐글 컴피티션 code 카테고리 캐글 컴피티션 code 탭에서 baseline을 검색하면, 많은 사람들이 올려놓은 기본 코드라인을 볼 수 있다. scratch baseline 실제로 사용하는 모델들은 어떤 식으로 구현됐는지 알 수 있다.","link":"/2022/09/24/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BD%94%EC%8A%A4-1%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0/"},{"title":"지원 배경 및 후기","text":"지원 배경 2022 전기 대학원에 떨어지고 개인적으로도 정말 힘든 일이 많았다. 처음 대학원이 떨어졌을 땐 포트폴리오를 제대로 만들고 부족한 부분도 공부하고 다시 지원하려 했다. 하지만 정신차리고 다시 마음을 다잡았을 때는 8월 말이었다. 공부를 하긴 했지만 그걸 블로그 등에 남긴 것도 아니었고 프로젝트 경험도 전혀 늘은 게 없었다. 컨택을 하기 위해 준비했던 건데 돌아보니 달라진게 없는 기분이었다. 이쯤 되자 정말 내가 대학원에 가고 싶은지, 데이터 관련 분야에서 일하고 싶은지 돌아보게 되었다. 결국 확신을 갖고 싶어 아무 부트캠프라도 들어가 프로젝트와 공부를 하고 싶었다. 사실 그전에는 부트캠프에 대해 부정적인 인식이 강했는데, 비전공자들이나 듣는 과정이라고 생각했기 때문이다. 하지만 냉정하게 내 자신을 평가했을 때 비전공자와 다를 바 없다는 생각이 들었고 부트캠프에 대해 알아보게 되었다. 고려사항 AI 트랙으로 부트캠프를 하는 곳은 많지 않았다. 네이버, 프로그래머스, 패스트캠퍼스, 엘리스코딩 등이 있었다. 다양한 후기를 찾아보고나서 내가 고려한 부분은 다음과 같았다. 코딩 테스트를 보는가 테스트를 준비하고 합격하는 사람들이 곧 그만큼 노력하고 열정이 있는 사람들일거라고 생각했다. 또, 팀 단위 과정이기 때문에 어느정도 수준이 있는 사람들과 같이 하고 싶었다. 대학원 다 떨어진 마당에 이걸 바라는게 웃기긴 하지만. AI를 서비스화하는 단계를 얼마나 다루는가 ML Ops 과정에 대해 배워보고 싶었다. 어떤 분야인지 궁금했다. 백엔드에 대한 부분을 얼마나 다루는가 ML을 서비스 하는 부분은 백엔드 지식이 필수다. 하지만 AI 트랙이기 때문에 이 부분이 주객전도가 일어나면 안된다고 생각했다. 백엔드만 주구장창 배운다는 느낌이 드는 곳은 피하고 싶었다. 공부를 하고 싶은 분위기를 만들어주는가 가장 중요하게 생각한 부분이다. 당연히 알아서 열심히 해야하지만, 그 환경을 잘 만들어주는 건 또 다른 이야기다. 가고 싶었던 곳 다양한 소개글, 영상, 후기를 봤을 때 가장 붙고 싶었던 곳은 네이버에서 진행하는 Boostcoursse - AI Tech와 프로그래머스에서 진행하는 Dev course였다. 부스트코스는 특히 참가한 사람들의 학습 환경을 만들어주기 위해 노력하는 것 같았다. 온보딩 키트라던지, 다양한 커뮤니티 이벤트라던지.. 또한 신청을 받으면서 시험 대비용 프리코스(Pre-course)를 열어줬는데 이 강의 퀄리티가 정말 좋았다. 혼자서 공부하던 내용들이 쭉 정리가 되는 느낌이었다. 알아봤을 때는 부스트코스의 지원기간이 일주일 정도 남아있었다. 어떻게든 자기소개서를 작성하고 노션에 포트폴리오를 만들었다. 노션 포트폴리오 구글링을 하고 그 중에서 가장 맘에 드는 템플릿으로 작성했다. 사실 담을 내용이 없어서 만들지말까 고민했지만, 어떻게든 어필하고 싶었다. 텅텅빈 깃허브와 스터디를 진행하면서 요약했던 것들을 노션에 담아 올렸다. 1차그 후,1차 테스트 볼 때는 프리코스를 다 듣진 못한채로 응시했다. 그래도 최대한 내용을 공부하고 문제를 풀 수 있을 수준으로 들었다고 생각했다. 근데 시험 창에 접속하자마자 좆됐다는 생각이 들었다. 1차에도 코테가 있었던 것이다. ‘태어나서 코테 공부 해본적이 없는데 어떡하지’라는 생각과 ‘시발 진짜 난 병신인가’하는 생각을 하면서 응시했다. 다행히 1차에서 보는 코테는 구현 문제 위주였다. 학교에서 내준 과제 수준이거나 그것보다 쉬웠기 때문에 열심히 풀었다. 아마 5문제 중에 3.5솔 정도 했던 것 같다. 오히려 생각보다 이론 부분이 쉽지 않았다. 지금 봐도 다 맞추진 못할 것 같다. 어찌어찌 1차를 치루고 오픈톡방 반응을 봤을 때, 난 딱 커트라인 수준이었기 때문에 조졌다 싶었다. 그래도 붙었을 때를 대비해서 2차 코딩 테스트를 준비했다. ‘이것이 코딩 테스트다’ 책을 보면서 동시에 오픈톡방에서 코테 스터디를 구해서 같이 프로그래머스 문제를 풀었다. 지금 확인해보니 lv.1은 39문제, lv.2는 12문제를 풀었다. 이걸 다 맞춘 건 아니고 lv.2 는 6문제 정도밖에 못 맞췄다.이코테 책은 끝까지 볼 필요가 없었고 볼 수도 없었다. 지금 다시 생각해보면 어떻게 붙었지? 싶다. 어쨌든 열심히 준비하는 와중에 1차 합격 메일을 받았고 힘을 내서 2차를 준비했다. 2차2차 시험을 보기 전날까지도 BFS/DFS는 풀지 못했고 DP는 한 두개 겨우 푸는 수준이었다. 긴장한 채로 응시했는데 내가 수준이 낮아서 그런지 대부분 구현 문제로 보였다. 마지막 문제정도가 DP였던 것 같다. DP정도 제외하면 조건도 널널했기 때문에 빡구현으로 해결할 수 있었다. 그럼 난 몇솔을 했을까? 7문제 중에 4.5솔을 했다. 다른 합격 후기글을 보면 5개는 풀어야 안정권이라는데 4.5솔을 했다. 시험보고 오픈톡방 반응도 비슷했다. 7솔한 사람도 많았고 대부분이 5솔은 했다. 조졌다고 생각하고 마음을 접고 프로그래머스 데브코스를 준비했다. 합격데브코스는 자기소개서도 빡세고 문제도 어렵다고 들어서 솔직히 준비할 때부터 쥰내 하기 싫었다. 부스트코스 때 열심히 했는데 떨어진 것도 영향이 있었고. 그냥 어떻게든 되겠지 하는 마음으로 침착맨이나 봤다. 근데 데브코스 코테 전날에 부스트코스 합격 메일이 왔다. 다행이다 싶었다 진짜. 이제 불태우는 일만 남았다고 생각하고 파이토치 스터디도 구했다. 코스가 시작되기 전까지 파이토치 스터디를 빡세게 진행하고 마침내 코스를 듣게 됐다.","link":"/2022/10/01/%EC%A7%80%EC%9B%90%ED%95%98%EA%B2%8C-%EB%90%9C-%EA%B3%84%EA%B8%B0/"}],"tags":[{"name":"부스트코스, boostcourse","slug":"부스트코스-boostcourse","link":"/tags/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BD%94%EC%8A%A4-boostcourse/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"}],"categories":[{"name":"Boostcourse AI Tech 4기","slug":"Boostcourse-AI-Tech-4기","link":"/categories/Boostcourse-AI-Tech-4%EA%B8%B0/"},{"name":"PyTorch","slug":"PyTorch","link":"/categories/PyTorch/"}],"pages":[{"title":"about","text":"","link":"/about/index.html"}]}